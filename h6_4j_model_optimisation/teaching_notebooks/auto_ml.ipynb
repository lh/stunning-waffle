{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flaml library from Microsoft can automate the process of selecting a model and appropriate hyperparameters. \n",
    "\n",
    "It is just one of the automated machine learning libraries out there!\n",
    "\n",
    "First, let's import the flaml library, along with our standard imports for data manipulation and machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from flaml import AutoML\n",
    "except ModuleNotFoundError:\n",
    "    !pip install flaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning setup imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model imports\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model scoring imports\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, f1_score, precision_score, \\\n",
    "                            recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to work with the preprocessed titanic data, splitting it into training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"data/processed_data.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "\n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)\n",
    "\n",
    "data = data.astype(float)\n",
    "\n",
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Dataset Samples: {len(X_train)}\")\n",
    "print(f\"Validation Dataset Samples: {len(X_validate)}\")\n",
    "print(f\"Testing Dataset Samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple initial auto ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a simple instance of automated training with flaml. \n",
    "\n",
    "We need to pass in training and testing data, tell it what kind of task we're conducting (e.g. classification, regression), how long to keep trying different models (here, we've gone for 60 seconds), and set a random seed for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train,\n",
    "           task=\"classification\",\n",
    "           time_budget=60,\n",
    "           seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this line to see what model it selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this will give us the parameters it chose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can output the best configuration for each of the estimators it tried. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_config_per_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate this model and put the results into a dataframe.\n",
    "\n",
    "We can use our `automl` variable where we'd usually use `model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = automl.predict(X_train)\n",
    "y_pred_val = automl.predict(X_validate)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "          'Accuracy (training)': np.mean(y_pred_train == y_train),\n",
    "          'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n",
    "          'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n",
    "          'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n",
    "          \"AUC\": roc_auc_score(y_validate, y_pred_val),\n",
    "          \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n",
    "          \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n",
    "          \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n",
    "          \"FP\": fp,\n",
    "          \"FN\": fn\n",
    "\n",
    "          }, index=[\"Auto ML - Default Parameters - Scoring on ROC AUC\"]\n",
    ").round(3)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like pretty reasonable performance, based on our previous interactions with the titanic dataset, though not as good as we've seen sometimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will allow us to quickly calculate and store metrics when assessing the `automl` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_ml_get_results(name):\n",
    "    y_pred_train = automl.predict(X_train)\n",
    "    y_pred_val = automl.predict(X_validate)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "            'Accuracy (training)': np.mean(y_pred_train == y_train),\n",
    "            'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n",
    "            'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n",
    "            'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n",
    "            \"AUC\": roc_auc_score(y_validate, y_pred_val),\n",
    "            \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n",
    "            \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n",
    "            \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn\n",
    "\n",
    "            }, index=[name]\n",
    "    ).round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's  try training again, this time asking it to score on a different metric - the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train,\n",
    "           task=\"classification\",\n",
    "           time_budget=60,\n",
    "           metric=\"f1\",\n",
    "           seed=42)\n",
    "results_df = pd.concat(\n",
    "    [results_df,\n",
    "    auto_ml_get_results(name=\"Auto ML - Scoring on f1\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran it with the default settings previously (our first run), the fifth line of the output gave us the estimated time required to find an optimal model. \n",
    "\n",
    "> [flaml.automl.logger: 07-29 12:25:03] {2345} INFO - Estimated sufficient time budget=658s. Estimated necessary time budget=16s.\n",
    "\n",
    "Let's allocate this length of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "\n",
    "automl.fit(X_train, y_train,\n",
    "           task=\"classification\",\n",
    "           time_budget=658,\n",
    "           seed=42)\n",
    "\n",
    "results_df = pd.concat(\n",
    "    [results_df,\n",
    "    auto_ml_get_results(name=\"Auto ML - scoring on ROC AUC - Training for ~11 minutes\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our updated results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare this with some other ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to fit and train any provided model, then return the required metrics to be added onto our table from above.\n",
    "\n",
    "This will allow us to compare the performance of flaml with specifying the model ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_train(name=\"XGBoost\", X_train=X_train, X_validate=X_validate,\n",
    "              y_train=y_train, y_validate=y_validate,\n",
    "              model=XGBClassifier(random_state=42)\n",
    "              ):\n",
    "\n",
    "     model.fit(X_train, y_train)\n",
    "\n",
    "     y_pred_train = model.predict(X_train)\n",
    "     y_pred_val = model.predict(X_validate)\n",
    "\n",
    "     tn, fp, fn, tp = confusion_matrix(y_validate, y_pred_val, labels=[0, 1]).ravel()\n",
    "\n",
    "     return pd.DataFrame({\n",
    "            'Accuracy (training)': np.mean(y_pred_train == y_train),\n",
    "            'Accuracy (validation)': np.mean(y_pred_val == y_validate),\n",
    "            'Precision (validation)': precision_score(y_validate, y_pred_val, average='macro'),\n",
    "            'Recall (validation)': recall_score(y_validate, y_pred_val, average='macro'),\n",
    "            \"AUC\": roc_auc_score(y_validate, y_pred_val),\n",
    "            \"Training AUC\": roc_auc_score(y_train, y_pred_train),\n",
    "            \"f1\": f1_score(y_validate, y_pred_val, average='macro'),\n",
    "            \"Training f1\": f1_score(y_train, y_pred_train, average='macro'),\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn\n",
    "          }, index=[name]\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to quickly assess the performance of a range of other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat(\n",
    "    [results_df,\n",
    "     fit_train(), # This uses the default - xgboost\n",
    "     fit_train(name=\"Decision Tree (Defaults)\", model=DecisionTreeClassifier()),\n",
    "     fit_train(name=\"Random Forest (Defaults)\", model=RandomForestClassifier(random_state=42)),\n",
    "     fit_train(name=\"LightGBM (Defaults)\", model=LGBMClassifier(random_state=42)),\n",
    "     fit_train(name=\"Catboost (Defaults)\", model=CatBoostClassifier(random_state=42, verbose=False)),\n",
    "\n",
    "     ]\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort this by validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(\"Accuracy (validation)\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another way to quickly compare the performance of the different models.\n",
    "\n",
    "We will rank each model by its performance against the other models, with a lower number indicating better performance relative to the other models (e.g. the model with the highest precision will be ranked 1; the model with the lowest number of false negatives will be ranked 1).\n",
    "\n",
    "We will omit training accuracy from our calculations as we are more interested in its likely 'real-world' performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df_high_good = results_df[['Accuracy (validation)', 'Precision (validation)', 'Recall (validation)', 'AUC', 'f1']].rank(method='dense', ascending=False).convert_dtypes()\n",
    "ranking_df_low_good = results_df[['FP', 'FN']].rank(method='dense', ascending=True).convert_dtypes()\n",
    "\n",
    "ranking_df = ranking_df_high_good.merge(ranking_df_low_good, left_index=True, right_index=True)\n",
    "\n",
    "ranking_df['Rank Sum'] = ranking_df.sum(axis=1)\n",
    "ranking_df = ranking_df.sort_values('Rank Sum', ascending=True).convert_dtypes()\n",
    "ranking_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could plot this output as well as everything is on the same scale (though we will omit the rank sum) as that's much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df.drop(columns=\"Rank Sum\").plot(\n",
    "    kind=\"barh\",\n",
    "    title=\"Performance Ranking (Higher Rank Value = Worse)\",\n",
    "    xlabel=\"Rank of Performance\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
