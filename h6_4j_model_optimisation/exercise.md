Your challenge for the dataset you have chosen is to try and train a model and provide supporting evidence to show you’ve optimised your model in some manner and checked how good your model is.

You may wish to split tasks up instead of all working together simultaneously on the same parts. 

Be careful around imbalanced classes if this is something that comes up in your dataset. 
You could consider using the techniques from Dan’s session on SMOTE or what we’ve covered today.

You can use any model we’ve covered, including deep learning networks.
You could just choose one algorithm and focus on that, or you could compare the performance of multiple algorithms. 

At the end of tomorrow (Wednesday), you will submit your notebooks and I will award some ad-hoc points to the best* notebooks - I’m more interested in good comments and exploration than raw performance.

Try to include at least 2 of the following:

- Feature Engineering
- Feature Selection
- Missing Data Imputation
- Hyperparameter tuning via grid search or Optuna
- A voting ensemble
- Using a model parameter for dealing with Imbalanced Data
- AutoML
- Pipelines
- Calibration Plots 

Consider these areas from previous sessions too:

- A function to simplify evaluating models
- Some simple exploratory data analysis of the dataset (e.g. - counts of different values, visualising distributions, etc.)
- A range of model metrics (i.e. not just accuracy - consider - precision, recall, ROC, AUC, confusion matrices etc. or - MAPE/MAE etc.)
- Some plots that explain your model’s decisions (e.g. - feature importance, SHAP, ICE or PDP)
- A discussion of any ethical considerations for implementation or other issues with your model/dataset

