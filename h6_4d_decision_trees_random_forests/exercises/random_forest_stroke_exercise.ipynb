{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSMA Exercise\n",
    "\n",
    "The data loaded in this exercise is for seven acute stroke units, and whether a patient receives clost-busting treatment for stroke.  There are lots of features, and a description of the features can be found in the file stroke_data_feature_descriptions.csv.\n",
    "\n",
    "Train a decision tree model to try to predict whether or not a stroke patient receives clot-busting treatment.  Use the prompts below to write each section of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to import the dataset and the libraries we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import preprocessing functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import machine learning model of interest\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# Import package to investigate our loaded dataframe\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Import functions for evaluating model\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report, \\\n",
    "                            confusion_matrix, ConfusionMatrixDisplay, auc, roc_curve\n",
    "from sklearn.metrics import auc, roc_curve, RocCurveDisplay, f1_score, precision_score, \\\n",
    "                            recall_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "                            classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Imports relating to logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Imports relating to plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Download data\n",
    "# (not required if running locally and have previously downloaded data)\n",
    "\n",
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "\n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '2004_titanic/master/jupyter_notebooks/data/hsma_stroke.csv'\n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data to data subfolder\n",
    "    data.to_csv(data_directory + 'hsma_stroke.csv', index=False)\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data/hsma_stroke.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at an overview of the data. Choose whichever method you like.\n",
    "\n",
    "(e.g. something like the 'head' or 'describe' method from pandas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the main stroke dataset into features and labels.\n",
    "\n",
    "Remember - we're trying to predict whether patients are given clotbusting treatment or not.\n",
    "\n",
    "What column contains that information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Clotbuster given', axis=1)\n",
    "y = data['Clotbuster given'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets. \n",
    "\n",
    "Start with a train/test split of 80/20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_display(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "\n",
    "train_and_display(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier() # Create a Decision Tree Model\n",
    "model = model.fit(X_train,y_train) # Fit the model using our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to predict labels in both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and compare accuracy across training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "\n",
    "print(f\"Accuracy of predicting training data = {accuracy_train:.3%}\")\n",
    "print(f\"Accuracy of predicting testing data = {accuracy_test:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the other model metrics.\n",
    "\n",
    "- precision\n",
    "- specificity\n",
    "- recall (sensitivity)\n",
    "- f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score_test = precision_score(y_test, y_pred_test)\n",
    "recall_sensitivity_score_test = recall_score(y_test, y_pred_test)\n",
    "specificity_score_test = precision_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Precision score for testing data = {precision_score_test:.3%}\")\n",
    "print(f\"Recall (sensitivity) score for testing data = {recall_sensitivity_score_test:.3%}\")\n",
    "print(f\"Specificity score for testing data = {specificity_score_test:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_test, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this using the `classification_report` function, returning the output as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(\n",
    "    y_true = y_train,\n",
    "    y_pred = y_pred_train,\n",
    "    target_names=[\"Not Given Clotbuster\", \"Given Clotbuster\"],\n",
    "    output_dict= True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test\n",
    "        ),\n",
    "        display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix_dt.plot(ax=ax)\n",
    "ax.title.set_text('Decision Tree')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a normalized confusion matrix for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test, \n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Not Given Clotbuster\", \"Given Clotbuster\"]\n",
    "        \n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "\n",
    "\n",
    "confusion_matrix_dt.plot(ax=ax)\n",
    "ax.title.set_text('Decision Tree')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Refining Your Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment by changing a few parameters.\n",
    "\n",
    "After changing the parameters, look at the model metrics like accuracy, precision, and recall.\n",
    "\n",
    "Tweak the parameters to see what model performance you can achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "\n",
    "for depth in range(1, 21):\n",
    "    model = RandomForestClassifier(max_depth=depth) \n",
    "    score = train_and_display(model, X_train, y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(range(1, 21), scores)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score vs. Max Depth')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "\n",
    "for n_estimators in range(1, 21):\n",
    "    model = RandomForestClassifier(n_estimators) \n",
    "    score = train_and_display(model, X_train, y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(range(1, 21), scores)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score vs. Number of trees in the forest')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Comparing Performance with a Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your code in from the previous exercise on decision trees.\n",
    "\n",
    "If you tuned your decision tree, you can bring in the best-performing of your decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier() \n",
    "\n",
    "train_and_display(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all of the metrics.\n",
    "\n",
    "- precision\n",
    "- specificity\n",
    "- recall (sensitivity)\n",
    "- f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this using the `classification_report` function, returning the output as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_report(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict)\n",
    "    return report_df\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "report_df = train_and_report(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix for the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a normalised confusion matrix for the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create receiver operating curves (ROC), labelled with the area under the curve (AUC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_report(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict)\n",
    "    roc_curve = RocCurveDisplay.from_estimator(\n",
    "    model, X_test, y_test\n",
    ")\n",
    "\n",
    "    fig = roc_curve.figure_\n",
    "    x = roc_curve.ax_\n",
    "    ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    return report_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Performance with a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your code in from last week's logistic regression exercise. \n",
    "\n",
    "**Remember - you will need to standardise the data for the logistic regression model!**\n",
    "\n",
    "Look at all of the metrics.\n",
    "\n",
    "- precision\n",
    "- specificity\n",
    "- recall (sensitivity)\n",
    "- f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix for the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a normalised confusion matrix for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we compared the performance of the logistic regression model and the decision tree model.\n",
    "\n",
    "Now consider the random forest too. \n",
    "\n",
    "Compare and contrast the confusion matrices for each of these.\n",
    "\n",
    "If one of these models were to be selected, which model would you recommend to put into use, and why?\n",
    "\n",
    "Remember: giving thrombolysis to good candidates for it can lead to less disability after stroke and improved outcomes. However, there is a risk that giving thrombolysis to the wrong person could lead to additional bleeding on the brain and worse outcomes. What might you want to balance?\n",
    "\n",
    "You can write your answer into the empty cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting all of your confusion matrices onto a single matplotlib figure. \n",
    "\n",
    "Make sure you give each of these a title.\n",
    "\n",
    "Hint: You'll need to create multiple matplotlib subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for the normalised confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same for your ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a random forest gives us another way to look at feature importance in our datasets.\n",
    "\n",
    "Take a look at this example from the scikit learn documentation. \n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "Calculate the feature importance using both methods (mean decrease in impurity, and feature permutation) for the dataset we have been working with in this exercise. \n",
    "\n",
    "Do they produce the same ranking of feature importance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise 3\n",
    "Can you improve accuracy of your random forest by changing the size of your train / test split?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try dropping some features from your data.  \n",
    "\n",
    "Can you improve the performance of your random forest this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try these models and compare the results:\n",
    "\n",
    "AdaBoost\n",
    "XGBoost\n",
    "CatBoost \n",
    "You won’t see the full benefits of this as we’ve already one-hot encoded this dataset\n",
    "Histogram-based gradient boosting\n",
    "LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier()\n",
    "\n",
    "AdaBoostClassifier_scores = train_and_report(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "XGBClassifier_scores = train_and_report(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = LGBMClassifier()\n",
    "LGBMClassifier_scores = train_and_report(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "HistGradientBoostingClassifier_scores = train_and_report(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = AdaBoostClassifier()\n",
    "model2 = XGBClassifier()\n",
    "model3 = LGBMClassifier()\n",
    "model4 = HistGradientBoostingClassifier()\n",
    "model5 = RandomForestClassifier()\n",
    "\n",
    "\n",
    "\n",
    "models = [model1, model2, model3, model4, model5]\n",
    "model_names = ['AdaBoost', 'XGB', 'LGBM', 'HistGradientBoosting', 'RandomForest']\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "linestyles = ['-', '--', '-.', ':', '-']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model, name, color, linestyle in zip(models, model_names, colors, linestyles):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_score = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=color, linestyle=linestyle,\n",
    "             lw=2, label=f'{name} (area = {roc_auc:.2f})')\n",
    "\n",
    "# Plot ROC curve for a random model (chance)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
