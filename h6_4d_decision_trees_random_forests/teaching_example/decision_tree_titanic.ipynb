{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import machine learning methods\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, roc_curve, RocCurveDisplay, f1_score, precision_score, \\\n",
    "                            recall_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "                            classification_report, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "\n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "\n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='../datasets/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_titanic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/processed_titanic_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "# inplace=True means change the dataframe itself - don't create a copy with this column dropped\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into training and tets sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier() # Create a Decision Tree Model\n",
    "model = model.fit(X_train,y_train) # Fit the model using our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shorthand below says to check each predicted y value against the actual\n",
    "# y value in the training data.  This gives a list of True and False values\n",
    "# for each prediction, where True indicates the predicted value matches the\n",
    "# actual value.  Then we take the mean of these Boolean values, which gives\n",
    "# us a proportion (where if all values were True, the proportion would be 1.0)\n",
    "# If you want to see why that works, just uncomment the following line of code\n",
    "# to see what y_pred_train == y_train is doing.\n",
    "# print (y_pred_train == y_train)\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train:3f}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test:3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first ten predicted classes\n",
    "classes = model.predict(X_test)\n",
    "classes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first ten predicted probabilities\n",
    "probabilities = model.predict_proba(X_test)\n",
    "probabilities[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tree(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_tree(\n",
    "    model,\n",
    "    feature_names=data.drop('Survived',axis=1).columns.tolist(),\n",
    "    class_names=['Died', 'Survived'],\n",
    "    filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking DT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_run_dt(model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "    print (f'Accuracy of predicting training data = {accuracy_train:.3f}')\n",
    "    print (f'Accuracy of predicting test data = {accuracy_test:.3f}')\n",
    "    print(f'Precision on test data = {precision_score(y_test, y_pred_test, average=\"micro\"):.3f}')\n",
    "    print (f'Precision on test data = {precision_score(y_test, y_pred_test, average=\"micro\"):.3f}')\n",
    "    print (f'Recall on test data = {recall_score(y_test, y_pred_test, average=\"micro\"):.3f}')\n",
    "    print (f'Specificity on test data = {precision_score(y_test, y_pred_test, average=\"micro\", pos_label=0):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_test, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Samples Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier(min_samples_leaf=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(1, 15, 1):\n",
    "    model = DecisionTreeClassifier(min_samples_leaf=i, random_state=42)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'min_samples_leaf': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='min_samples_leaf'),\n",
    "        x='min_samples_leaf', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min Samples Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier(min_samples_split=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(2, 15, 1):\n",
    "    model = DecisionTreeClassifier(min_samples_split=i)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'min_samples_split': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='min_samples_split'),\n",
    "        x='min_samples_split', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier(max_depth=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier(max_depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(model = DecisionTreeClassifier(max_depth=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(1, 15, 1):\n",
    "    model = model = DecisionTreeClassifier(max_depth=i)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'max_depth': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='max_depth'),\n",
    "        x='max_depth', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "tree_plot = plot_tree(model,\n",
    "    feature_names=data.drop('Survived',axis=1).columns.tolist(),\n",
    "    class_names=['Died', 'Survived'],\n",
    "    filled=True,\n",
    "    ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "tree_plot = plot_tree(model,\n",
    "    feature_names=data.drop('Survived',axis=1).columns.tolist(),\n",
    "    class_names=['Died', 'Survived'],\n",
    "    filled=True,\n",
    "    ax=ax,\n",
    "    fontsize=11\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with our log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "\n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.fit_transform(X_train)\n",
    "    test_std=sc.fit_transform(X_test)\n",
    "\n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_standardised, X_test_standardised = standardise_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_standardised,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train_standardised)\n",
    "y_pred_test = model.predict(X_test_standardised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shorthand below says to check each predicted y value against the actual\n",
    "# y value in the training data.  This gives a list of True and False values\n",
    "# for each prediction, where True indicates the predicted value matches the\n",
    "# actual value.  Then we take the mean of these Boolean values, which gives\n",
    "# us a proportion (where if all values were True, the proportion would be 1.0)\n",
    "# If you want to see why that works, just uncomment the following line of code\n",
    "# to see what y_pred_train == y_train is doing.\n",
    "# print (y_pred_train == y_train)\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best train accuracy seen in DT: 0.823\n",
    "Train accuracy seen in LR: 0.805\n",
    "\n",
    "Best test accuracy seen in DT: 0.820\n",
    "Test accuracy seen in LR: 0.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ranvir.xyz/blog/practical-approach-to-tree-pruning-using-sklearn/\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas, impurities)\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total impurity of leaves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depths = [model.tree_.max_depth for model in models]\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.plot(ccp_alphas[:-1], tree_depths[:-1])\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total depth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_scores = [accuracy_score(y_test, model.predict(X_test)) for model in models]\n",
    "\n",
    "tree_depths = [model.tree_.max_depth for model in models]\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(ccp_alphas[:-1], acc_scores[:-1])\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"Accuracy scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final model from this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_run_dt(DecisionTreeClassifier(random_state=0, ccp_alpha=0.0045))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring metrics in our lr and dt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = model = DecisionTreeClassifier(max_depth=6)\n",
    "decision_tree_model = decision_tree_model.fit(X_train,y_train)\n",
    "y_pred_train_dt = decision_tree_model.predict(X_train)\n",
    "y_pred_test_dt = decision_tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve = RocCurveDisplay.from_estimator(\n",
    "    decision_tree_model, X_test, y_test\n",
    ")\n",
    "\n",
    "fig = roc_curve.figure_\n",
    "ax = roc_curve.ax_\n",
    "\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(max_depth=6)\n",
    "decision_tree_model = decision_tree_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train_dt = decision_tree_model.predict(X_train)\n",
    "y_pred_test_dt = decision_tree_model.predict(X_test)\n",
    "\n",
    "roc_curve_dt = RocCurveDisplay.from_estimator(\n",
    "    decision_tree_model, X_test, y_test\n",
    ")\n",
    "\n",
    "fig = roc_curve_dt.figure_\n",
    "ax = roc_curve_dt.ax_\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_dt\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_dt.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dt_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_dt,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_dt_normalised.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(precision_recall_fscore_support(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_dt,\n",
    "        average=\"binary\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardise_data(X_train, X_test):\n",
    "\n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.fit_transform(X_train)\n",
    "    test_std=sc.fit_transform(X_test)\n",
    "\n",
    "    return train_std, test_std\n",
    "\n",
    "X_train_standardised, X_test_standardised = standardise_data(X_train, X_test)\n",
    "\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model = logistic_regression_model.fit(X_train_standardised,y_train)\n",
    "\n",
    "y_pred_train_lr = logistic_regression_model.predict(X_train_standardised)\n",
    "y_pred_test_lr = logistic_regression_model.predict(X_test_standardised)\n",
    "\n",
    "accuracy_train = np.mean(y_pred_train_lr == y_train)\n",
    "accuracy_test = np.mean(y_pred_test_lr == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_lr = RocCurveDisplay.from_estimator(\n",
    "    logistic_regression_model, X_test_standardised, y_test\n",
    ")\n",
    "\n",
    "fig = roc_curve_lr.figure_\n",
    "ax = roc_curve_lr.ax_\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_lr = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr,\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_lr.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_lr_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr,\n",
    "        normalize='true',\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_lr_normalised.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(classification_report(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr,\n",
    "        target_names=[\"Died\", \"Survived\"],\n",
    "        output_dict=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fbeta, support = precision_recall_fscore_support(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr,\n",
    "        average=\"binary\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "confusion_matrix_dt.plot(ax=ax1)\n",
    "ax1.title.set_text('Decision Tree')\n",
    "\n",
    "confusion_matrix_lr.plot(ax=ax2)\n",
    "ax2.title.set_text('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "confusion_matrix_dt_normalised.plot(ax=ax1)\n",
    "ax1.title.set_text('Decision Tree - Normalised')\n",
    "\n",
    "confusion_matrix_lr_normalised.plot(ax=ax2)\n",
    "ax2.title.set_text('Logistic Regression - Normalised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "roc_curve_dt.plot(ax=ax1)\n",
    "ax1.title.set_text('Decision Tree')\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "\n",
    "roc_curve_lr.plot(ax=ax2)\n",
    "ax2.title.set_text('Logistic Regression')\n",
    "ax2.plot([0, 1], [0, 1], color='darkblue', linestyle='--')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
