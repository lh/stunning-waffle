{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import machine learning methods\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve, RocCurveDisplay, f1_score, precision_score, \\\n",
    "                            recall_score, confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "                            classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = False\n",
    "\n",
    "if download_required:\n",
    "\n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "\n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='../datasets/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_titanic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/processed_titanic_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Passengerid (axis=1 indicates we are removing a column rather than a row)\n",
    "# We drop passenger ID as it is not original data\n",
    "# inplace=True means change the dataframe itself - don't create a copy with this column dropped\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into training and tets sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state=42)\n",
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shorthand below says to check each predicted y value against the actual\n",
    "# y value in the training data.  This gives a list of True and False values\n",
    "# for each prediction, where True indicates the predicted value matches the\n",
    "# actual value.  Then we take the mean of these Boolean values, which gives\n",
    "# us a proportion (where if all values were True, the proportion would be 1.0)\n",
    "# If you want to see why that works, just uncomment the following line of code\n",
    "# to see what y_pred_train == y_train is doing.\n",
    "# print (y_pred_train == y_train)\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first ten predicted classes\n",
    "classes = model.predict(X_test)\n",
    "classes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first ten predicted probabilities\n",
    "probabilities = model.predict_proba(X_test)\n",
    "probabilities[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_xg = f1_score(y_test, y_pred_test, average='macro')\n",
    "precision_score_xg = precision_score(y_test, y_pred_test, average='macro')\n",
    "recall_score_xg = recall_score(y_test, y_pred_test, average='macro')\n",
    "\n",
    "print (f'Accuracy of predicting test data = {accuracy_test}')\n",
    "print (f'f1 score = {f1_score_xg}')\n",
    "print (f'precision score = {recall_score_xg}')\n",
    "print (f'recall score = {recall_score_xg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_run(model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "    print (f'Accuracy of predicting training data = {accuracy_train:.3f}')\n",
    "    print (f'Accuracy of predicting test data = {accuracy_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_and_run(model = LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_and_run(model = DecisionTreeClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_and_run(model = RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_and_run(model = XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "random_forest_model = RandomForestClassifier()\n",
    "random_forest_model = random_forest_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train_rf = random_forest_model.predict(X_train)\n",
    "y_pred_test_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "roc_curve_rf = RocCurveDisplay.from_estimator(\n",
    "    random_forest_model, X_test, y_test\n",
    ")\n",
    "\n",
    "confusion_matrix_rf = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_rf\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_rf_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_rf,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "decision_tree_model = DecisionTreeClassifier(max_depth=6)\n",
    "decision_tree_model = decision_tree_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train_dt = decision_tree_model.predict(X_train)\n",
    "y_pred_test_dt = decision_tree_model.predict(X_test)\n",
    "\n",
    "roc_curve_dt = RocCurveDisplay.from_estimator(\n",
    "    decision_tree_model, X_test, y_test\n",
    ")\n",
    "\n",
    "confusion_matrix_dt = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_dt\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_dt_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_dt,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def standardise_data(X_train, X_test):\n",
    "\n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.fit_transform(X_train)\n",
    "    test_std=sc.fit_transform(X_test)\n",
    "\n",
    "    return train_std, test_std\n",
    "\n",
    "X_train_standardised, X_test_standardised = standardise_data(X_train, X_test)\n",
    "\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train_standardised,y_train)\n",
    "\n",
    "y_pred_train_lr = logistic_regression_model.predict(X_train_standardised)\n",
    "y_pred_test_lr = logistic_regression_model.predict(X_test_standardised)\n",
    "\n",
    "roc_curve_lr = RocCurveDisplay.from_estimator(\n",
    "    logistic_regression_model, X_test_standardised, y_test\n",
    ")\n",
    "\n",
    "confusion_matrix_lr = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_lr_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_lr,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model = xgboost_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train_xg = xgboost_model.predict(X_train)\n",
    "y_pred_test_xg = xgboost_model.predict(X_test)\n",
    "\n",
    "roc_curve_xg = RocCurveDisplay.from_estimator(\n",
    "    xgboost_model, X_test, y_test\n",
    ")\n",
    "\n",
    "confusion_matrix_xg = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_xg\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n",
    "\n",
    "confusion_matrix_xg_normalised = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test_xg,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(14, 5))\n",
    "\n",
    "confusion_matrix_lr.plot(ax=ax1)\n",
    "ax1.title.set_text('Logistic Regression')\n",
    "\n",
    "confusion_matrix_dt.plot(ax=ax2)\n",
    "ax2.title.set_text('Decision Tree')\n",
    "\n",
    "confusion_matrix_rf.plot(ax=ax3)\n",
    "ax3.title.set_text('Random Forest')\n",
    "\n",
    "confusion_matrix_xg.plot(ax=ax4)\n",
    "ax4.title.set_text('XGBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(14, 5))\n",
    "confusion_matrix_rf_normalised.plot(ax=ax1)\n",
    "ax1.title.set_text('Random Forest - Normalised')\n",
    "\n",
    "confusion_matrix_dt_normalised.plot(ax=ax2)\n",
    "ax2.title.set_text('Decision Tree - Normalised')\n",
    "\n",
    "confusion_matrix_lr_normalised.plot(ax=ax3)\n",
    "ax3.title.set_text('Logistic Regression - Normalised')\n",
    "\n",
    "confusion_matrix_xg_normalised.plot(ax=ax4)\n",
    "ax4.title.set_text('XGBoost - Normalised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(14, 5))\n",
    "\n",
    "roc_curve_rf.plot(ax=ax1)\n",
    "ax1.title.set_text('Random Forest')\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "\n",
    "roc_curve_dt.plot(ax=ax2)\n",
    "ax2.title.set_text('Decision Tree')\n",
    "ax2.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "\n",
    "roc_curve_lr.plot(ax=ax3)\n",
    "ax3.title.set_text('Logistic Regression')\n",
    "ax3.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "\n",
    "roc_curve_xg.plot(ax=ax4)\n",
    "ax4.title.set_text('XGBoost')\n",
    "ax4.plot([0, 1], [0, 1], color='darkblue', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Parallel Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time a standard xgboost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "start_at = datetime.now()\n",
    "train_and_run(model = XGBClassifier())\n",
    "print(\"Duration =\", (datetime.now() - start_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time a parallel xgboost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "start_at = datetime.now()\n",
    "train_and_run(model = XGBClassifier(nthread=-1))\n",
    "print(\"Duration =\", (datetime.now() - start_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n estimators (trees per forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(10, 500, 10):\n",
    "    model = model = XGBClassifier(n_estimators=i, random_state=42, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n",
    "        x='n_estimators', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n estimators (trees per forest) - with max depth of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(10, 200, 10):\n",
    "    model = XGBClassifier(n_estimators=i, random_state=42, max_depth=5, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'n_estimators': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='n_estimators'),\n",
    "        x='n_estimators', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(accuracy_results).set_index(\"n_estimators\").sort_values(by=[\"accuracy_test\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "best_n_estimators = pd.DataFrame(accuracy_results).sort_values(by=[\"accuracy_test\"], ascending=False).head(1)['n_estimators'].values[0]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=best_n_estimators, random_state=42, max_depth=8)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "roc_curve = RocCurveDisplay.from_estimator(\n",
    "    model, X_test, y_test\n",
    ")\n",
    "\n",
    "fig = roc_curve.figure_\n",
    "ax = roc_curve.ax_\n",
    "\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ").plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred_test,\n",
    "        normalize='true'\n",
    "        ),\n",
    "        display_labels=[\"Died\", \"Survived\"]\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate (ETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in np.arange(0.005, 0.2, 0.01):\n",
    "    model = XGBClassifier(learning_rate=i, random_state=42, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'learning_rate': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='learning_rate'),\n",
    "        x='learning_rate', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in range(2, 15):\n",
    "    model = XGBClassifier(min_child_weight=i, random_state=42, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'min_child_weight': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='min_child_weight'),\n",
    "        x='min_child_weight', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in np.arange(0.05, 1, 0.05):\n",
    "    model = XGBClassifier(subsample=i, random_state=42, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'subsample': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='subsample'),\n",
    "        x='subsample', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "\n",
    "for i in np.arange(0.05, 1, 0.05):\n",
    "    model = XGBClassifier(colsample_bytree=i, random_state=42, nthread=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    accuracy_results.append({'accuracy_train': accuracy_train, 'accuracy_test': accuracy_test, 'colsample_bytree': i})\n",
    "\n",
    "px.line(pd.DataFrame(accuracy_results).melt(id_vars='colsample_bytree'),\n",
    "        x='colsample_bytree', y='value', color='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num boost round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram based boosting classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model = HistGradientBoostingClassifier()\n",
    "model.fit(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
