{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NumPy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set TensorFlow random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"datasets/Labelled_1997_2017/HanDeSeT.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['Concat'] = train_dataset['utt1'].fillna('') + ' ' + train_dataset['utt2'].fillna('') + ' ' + train_dataset['utt3'].fillna('') + ' ' + train_dataset['utt4'].fillna('') + ' ' + train_dataset['utt5'].fillna('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.dropna(subset=['Concat'])\n",
    "slimmed_train_dataset = train_dataset.drop (columns=['utt1', 'utt2', 'utt3', 'utt4', 'utt5', 'title', 'motion', 'id', 'party affiliation',])\n",
    "slimmed_train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp training data split (train, test, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = slimmed_train_dataset[['manual speech', 'Concat']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "train_dataset, test_dataset = train_test_split(test_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Display the first few rows of each set\n",
    "print(\"Training Set:\")\n",
    "print(train_dataset.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_dataset.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up simple model so we can test on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_dataset['Concat'])\n",
    "X_val = vectorizer.transform(val_dataset['Concat'])\n",
    "X_test = vectorizer.transform(test_dataset['Concat'])\n",
    "\n",
    "# Get the target values\n",
    "y_train = train_dataset['manual speech']\n",
    "y_val = val_dataset['manual speech']\n",
    "y_test = test_dataset['manual speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(classification_report(y_val, val_predictions))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "'''\n",
    "to load the model use \n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train nlp model\n",
    "If we are going to use the keras from the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "# Dropout\n",
    "dropout_1 = 0.2\n",
    "dropout_2 = dropout_1\n",
    "\n",
    "# Early stopping\n",
    "_min_delta=0.001\n",
    "_patience=1\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Extract the 'Concat' column for adaptation\n",
    "train_text = train_dataset['Concat'].values\n",
    "\n",
    "# Adapt the vectorize_layer to the training data\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "# Function to vectorize text from the DataFrame\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    label = tf.expand_dims(label, -1)  # Reshape label to (batch_size, 1) if necessary\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "# Convert the DataFrame to TensorFlow Dataset\n",
    "def dataframe_to_dataset(df):\n",
    "    return tf.data.Dataset.from_tensor_slices((df['Concat'].values, df['manual speech'].values))\n",
    "\n",
    "X_train = dataframe_to_dataset(train_dataset).map(vectorize_text)\n",
    "X_val = dataframe_to_dataset(val_dataset).map(vectorize_text)\n",
    "X_test = dataframe_to_dataset(test_dataset).map(vectorize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfmodel = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features+1, embedding_dim),\n",
    "    layers.Dropout(dropout_1),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(dropout_2),\n",
    "    layers.Dense(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=_min_delta,\n",
    "    patience=_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel_history = tfmodel.fit(\n",
    "    X_train,\n",
    "    validation_data=X_val,\n",
    "    epochs=epochs,\n",
    "    callbacks=earlystop_callback,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = tfmodel.evaluate(X_test)\n",
    "print (\"EVALUATION ON TEST DATASET\")\n",
    "print (f\"Loss : {loss:.2f}\")\n",
    "print (f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfmodel.save('rebels.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "tfmodel = load_model('rebels.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point break out to another group to use trained model on nthe unlabelled data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
