{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"datasets/Labelled_1997_2017/HanDeSeT.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['Concat'] = train_dataset['utt1'].fillna('') + ' ' + train_dataset['utt2'].fillna('') + ' ' + train_dataset['utt3'].fillna('') + ' ' + train_dataset['utt4'].fillna('') + ' ' + train_dataset['utt5'].fillna('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.dropna(subset=['Concat'])\n",
    "slimmed_train_dataset = train_dataset.drop (columns=['utt1', 'utt2', 'utt3', 'utt4', 'utt5', 'title', 'motion', 'id', 'party affiliation',])\n",
    "slimmed_train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp training data split (train, test, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = slimmed_train_dataset[['manual speech', 'Concat']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "train_dataset, test_dataset = train_test_split(test_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Display the first few rows of each set\n",
    "print(\"Training Set:\")\n",
    "print(train_dataset.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_dataset.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up simple model so we can test on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_dataset['Concat'])\n",
    "X_val = vectorizer.transform(val_dataset['Concat'])\n",
    "X_test = vectorizer.transform(test_dataset['Concat'])\n",
    "\n",
    "# Get the target values\n",
    "y_train = train_dataset['manual speech']\n",
    "y_val = val_dataset['manual speech']\n",
    "y_test = test_dataset['manual speech']\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "print(\"Validation Set Evaluation:\")\n",
    "print(classification_report(y_val, val_predictions))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "'''\n",
    "to load the model use \n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train nlp model\n",
    "If we are going to use the keras from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point break out to another group to use trained model on nthe unlabelled data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
